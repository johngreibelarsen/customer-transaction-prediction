{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the data\n",
    "\n",
    "#### What files are in the input folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of files: 3\n",
      "['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print('No. of files: {}'.format(str(len(os.listdir(\"./input\")))))\n",
    "print(os.listdir(\"./input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 observations and 202 features in train set.\n",
      "200000 observations and 201 features in test set.\n"
     ]
    }
   ],
   "source": [
    "#Loading Train and Test Data\n",
    "\n",
    "df_train = pd.read_csv(\"./input/train.csv\")\n",
    "df_test = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "print(\"{} observations and {} features in train set.\".format(df_train.shape[0], df_train.shape[1]))\n",
    "print(\"{} observations and {} features in test set.\".format(df_test.shape[0], df_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 202 entries, ID_code to var_199\n",
      "dtypes: float64(200), int64(1), object(1)\n",
      "memory usage: 308.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7   ...     var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266   ...      4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338   ...      7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  14.6155   ...      2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  14.9250   ...      4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  19.2514   ...     -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.100490</td>\n",
       "      <td>10.679914</td>\n",
       "      <td>-1.627622</td>\n",
       "      <td>10.715192</td>\n",
       "      <td>6.796529</td>\n",
       "      <td>11.078333</td>\n",
       "      <td>-5.065317</td>\n",
       "      <td>5.408949</td>\n",
       "      <td>16.545850</td>\n",
       "      <td>0.284162</td>\n",
       "      <td>...</td>\n",
       "      <td>3.234440</td>\n",
       "      <td>7.438408</td>\n",
       "      <td>1.927839</td>\n",
       "      <td>3.331774</td>\n",
       "      <td>17.993784</td>\n",
       "      <td>-0.142088</td>\n",
       "      <td>2.303335</td>\n",
       "      <td>8.908158</td>\n",
       "      <td>15.870720</td>\n",
       "      <td>-3.326537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.300653</td>\n",
       "      <td>3.040051</td>\n",
       "      <td>4.050044</td>\n",
       "      <td>2.640894</td>\n",
       "      <td>2.043319</td>\n",
       "      <td>1.623150</td>\n",
       "      <td>7.863267</td>\n",
       "      <td>0.866607</td>\n",
       "      <td>3.418076</td>\n",
       "      <td>3.332634</td>\n",
       "      <td>...</td>\n",
       "      <td>4.559922</td>\n",
       "      <td>3.023272</td>\n",
       "      <td>1.478423</td>\n",
       "      <td>3.992030</td>\n",
       "      <td>3.135162</td>\n",
       "      <td>1.429372</td>\n",
       "      <td>5.454369</td>\n",
       "      <td>0.921625</td>\n",
       "      <td>3.010945</td>\n",
       "      <td>10.438015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408400</td>\n",
       "      <td>-15.043400</td>\n",
       "      <td>2.117100</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>5.074800</td>\n",
       "      <td>-32.562600</td>\n",
       "      <td>2.347300</td>\n",
       "      <td>5.349700</td>\n",
       "      <td>-10.505500</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.093300</td>\n",
       "      <td>-2.691700</td>\n",
       "      <td>-3.814500</td>\n",
       "      <td>-11.783400</td>\n",
       "      <td>8.694400</td>\n",
       "      <td>-5.261000</td>\n",
       "      <td>-14.209600</td>\n",
       "      <td>5.960600</td>\n",
       "      <td>6.299300</td>\n",
       "      <td>-38.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.453850</td>\n",
       "      <td>-4.740025</td>\n",
       "      <td>8.722475</td>\n",
       "      <td>5.254075</td>\n",
       "      <td>9.883175</td>\n",
       "      <td>-11.200350</td>\n",
       "      <td>4.767700</td>\n",
       "      <td>13.943800</td>\n",
       "      <td>-2.317800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058825</td>\n",
       "      <td>5.157400</td>\n",
       "      <td>0.889775</td>\n",
       "      <td>0.584600</td>\n",
       "      <td>15.629800</td>\n",
       "      <td>-1.170700</td>\n",
       "      <td>-1.946925</td>\n",
       "      <td>8.252800</td>\n",
       "      <td>13.829700</td>\n",
       "      <td>-11.208475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.524750</td>\n",
       "      <td>-1.608050</td>\n",
       "      <td>10.580000</td>\n",
       "      <td>6.825000</td>\n",
       "      <td>11.108250</td>\n",
       "      <td>-4.833150</td>\n",
       "      <td>5.385100</td>\n",
       "      <td>16.456800</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>...</td>\n",
       "      <td>3.203600</td>\n",
       "      <td>7.347750</td>\n",
       "      <td>1.901300</td>\n",
       "      <td>3.396350</td>\n",
       "      <td>17.957950</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>2.408900</td>\n",
       "      <td>8.888200</td>\n",
       "      <td>15.934050</td>\n",
       "      <td>-2.819550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.758200</td>\n",
       "      <td>1.358625</td>\n",
       "      <td>12.516700</td>\n",
       "      <td>8.324100</td>\n",
       "      <td>12.261125</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.102900</td>\n",
       "      <td>2.937900</td>\n",
       "      <td>...</td>\n",
       "      <td>6.406200</td>\n",
       "      <td>9.512525</td>\n",
       "      <td>2.949500</td>\n",
       "      <td>6.205800</td>\n",
       "      <td>20.396525</td>\n",
       "      <td>0.829600</td>\n",
       "      <td>6.556725</td>\n",
       "      <td>9.593300</td>\n",
       "      <td>18.064725</td>\n",
       "      <td>4.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.315000</td>\n",
       "      <td>10.376800</td>\n",
       "      <td>19.353000</td>\n",
       "      <td>13.188300</td>\n",
       "      <td>16.671400</td>\n",
       "      <td>17.251600</td>\n",
       "      <td>8.447700</td>\n",
       "      <td>27.691800</td>\n",
       "      <td>10.151300</td>\n",
       "      <td>...</td>\n",
       "      <td>18.440900</td>\n",
       "      <td>16.716500</td>\n",
       "      <td>8.402400</td>\n",
       "      <td>18.281800</td>\n",
       "      <td>27.928800</td>\n",
       "      <td>4.272900</td>\n",
       "      <td>18.321500</td>\n",
       "      <td>12.000400</td>\n",
       "      <td>26.079100</td>\n",
       "      <td>28.500700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              target          var_0          var_1          var_2  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.100490      10.679914      -1.627622      10.715192   \n",
       "std         0.300653       3.040051       4.050044       2.640894   \n",
       "min         0.000000       0.408400     -15.043400       2.117100   \n",
       "25%         0.000000       8.453850      -4.740025       8.722475   \n",
       "50%         0.000000      10.524750      -1.608050      10.580000   \n",
       "75%         0.000000      12.758200       1.358625      12.516700   \n",
       "max         1.000000      20.315000      10.376800      19.353000   \n",
       "\n",
       "               var_3          var_4          var_5          var_6  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        6.796529      11.078333      -5.065317       5.408949   \n",
       "std         2.043319       1.623150       7.863267       0.866607   \n",
       "min        -0.040200       5.074800     -32.562600       2.347300   \n",
       "25%         5.254075       9.883175     -11.200350       4.767700   \n",
       "50%         6.825000      11.108250      -4.833150       5.385100   \n",
       "75%         8.324100      12.261125       0.924800       6.003000   \n",
       "max        13.188300      16.671400      17.251600       8.447700   \n",
       "\n",
       "               var_7          var_8      ...              var_190  \\\n",
       "count  200000.000000  200000.000000      ...        200000.000000   \n",
       "mean       16.545850       0.284162      ...             3.234440   \n",
       "std         3.418076       3.332634      ...             4.559922   \n",
       "min         5.349700     -10.505500      ...           -14.093300   \n",
       "25%        13.943800      -2.317800      ...            -0.058825   \n",
       "50%        16.456800       0.393700      ...             3.203600   \n",
       "75%        19.102900       2.937900      ...             6.406200   \n",
       "max        27.691800      10.151300      ...            18.440900   \n",
       "\n",
       "             var_191        var_192        var_193        var_194  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        7.438408       1.927839       3.331774      17.993784   \n",
       "std         3.023272       1.478423       3.992030       3.135162   \n",
       "min        -2.691700      -3.814500     -11.783400       8.694400   \n",
       "25%         5.157400       0.889775       0.584600      15.629800   \n",
       "50%         7.347750       1.901300       3.396350      17.957950   \n",
       "75%         9.512525       2.949500       6.205800      20.396525   \n",
       "max        16.716500       8.402400      18.281800      27.928800   \n",
       "\n",
       "             var_195        var_196        var_197        var_198  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean       -0.142088       2.303335       8.908158      15.870720   \n",
       "std         1.429372       5.454369       0.921625       3.010945   \n",
       "min        -5.261000     -14.209600       5.960600       6.299300   \n",
       "25%        -1.170700      -1.946925       8.252800      13.829700   \n",
       "50%        -0.172700       2.408900       8.888200      15.934050   \n",
       "75%         0.829600       6.556725       9.593300      18.064725   \n",
       "max         4.272900      18.321500      12.000400      26.079100   \n",
       "\n",
       "             var_199  \n",
       "count  200000.000000  \n",
       "mean       -3.326537  \n",
       "std        10.438015  \n",
       "min       -38.852800  \n",
       "25%       -11.208475  \n",
       "50%        -2.819550  \n",
       "75%         4.836800  \n",
       "max        28.500700  \n",
       "\n",
       "[8 rows x 201 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 201 entries, ID_code to var_199\n",
      "dtypes: float64(200), object(1)\n",
      "memory usage: 306.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1556</td>\n",
       "      <td>11.8495</td>\n",
       "      <td>-1.4300</td>\n",
       "      <td>2.4508</td>\n",
       "      <td>13.7112</td>\n",
       "      <td>2.4669</td>\n",
       "      <td>4.3654</td>\n",
       "      <td>10.7200</td>\n",
       "      <td>15.4722</td>\n",
       "      <td>-8.7197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>10.6165</td>\n",
       "      <td>8.8349</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>10.1282</td>\n",
       "      <td>15.5765</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>-1.4852</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>19.1293</td>\n",
       "      <td>-20.9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7484</td>\n",
       "      <td>10.9935</td>\n",
       "      <td>1.9803</td>\n",
       "      <td>2.1800</td>\n",
       "      <td>12.9813</td>\n",
       "      <td>2.1281</td>\n",
       "      <td>-7.1086</td>\n",
       "      <td>7.0618</td>\n",
       "      <td>19.8956</td>\n",
       "      <td>-23.1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5702</td>\n",
       "      <td>9.0766</td>\n",
       "      <td>1.6580</td>\n",
       "      <td>3.5813</td>\n",
       "      <td>15.1874</td>\n",
       "      <td>3.1656</td>\n",
       "      <td>3.9567</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>13.0168</td>\n",
       "      <td>-4.2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2259</td>\n",
       "      <td>9.1723</td>\n",
       "      <td>1.2835</td>\n",
       "      <td>3.3778</td>\n",
       "      <td>19.5542</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-5.1612</td>\n",
       "      <td>7.2882</td>\n",
       "      <td>13.9260</td>\n",
       "      <td>-9.1846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    var_0    var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  test_0  11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493   \n",
       "1  test_1   8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196   \n",
       "2  test_2   5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950   \n",
       "3  test_3   8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397   \n",
       "4  test_4  11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595   \n",
       "\n",
       "     var_7   var_8   ...     var_190  var_191  var_192  var_193  var_194  \\\n",
       "0  18.2675  2.1337   ...     -2.1556  11.8495  -1.4300   2.4508  13.7112   \n",
       "1  18.6316 -4.4131   ...     10.6165   8.8349   0.9403  10.1282  15.5765   \n",
       "2  20.2537  1.5233   ...     -0.7484  10.9935   1.9803   2.1800  12.9813   \n",
       "3  20.5660  3.3755   ...      9.5702   9.0766   1.6580   3.5813  15.1874   \n",
       "4  10.6048  2.9890   ...      4.2259   9.1723   1.2835   3.3778  19.5542   \n",
       "\n",
       "   var_195  var_196  var_197  var_198  var_199  \n",
       "0   2.4669   4.3654  10.7200  15.4722  -8.7197  \n",
       "1   0.4773  -1.4852   9.8714  19.1293 -20.9760  \n",
       "2   2.1281  -7.1086   7.0618  19.8956 -23.1794  \n",
       "3   3.1656   3.9567   9.2295  13.0168  -4.2108  \n",
       "4  -0.2860  -5.1612   7.2882  13.9260  -9.1846  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the target score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(df_train.target.values, bins=500)\n",
    "plt.title('Histogram target counts')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    179902\n",
       "1     20098\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's centre and normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(copy=True, feature_range=(-1, 1))\n",
    "x_transformed_train = scaler.fit_transform(df_train.iloc[:,2:])\n",
    "x_transformed_test = scaler.transform(df_test.iloc[:,1:])\n",
    "\n",
    "df_transformed_train = pd.DataFrame(data=x_transformed_train)\n",
    "df_transformed_test = pd.DataFrame(data=x_transformed_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 200)\n",
      "(5000, 200)\n"
     ]
    }
   ],
   "source": [
    "df_train_small = df_train.iloc[:5000,2:]\n",
    "df_test_small = df_test.iloc[:5000,1:]\n",
    "\n",
    "print(df_train_small.shape)\n",
    "print(df_test_small.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading more features for train dataset\n",
      "Creating Metrics Part 1\n",
      "  * Loading new data metrics:  _1\n",
      "  * Loading moving average metric:  _1\n",
      "  * Loading percentiles:  _1\n",
      "Creating Metrics Part 2\n",
      "  * Loading new data metrics:  _2\n",
      "  * Loading moving average metric:  _2\n",
      "  * Loading percentiles:  _2\n",
      "--------------------------------------------------\n",
      "Loading more features for test dataset\n",
      "Creating Metrics Part 1\n",
      "  * Loading new data metrics:  _1\n",
      "  * Loading moving average metric:  _1\n",
      "  * Loading percentiles:  _1\n",
      "Creating Metrics Part 2\n",
      "  * Loading new data metrics:  _2\n",
      "  * Loading moving average metric:  _2\n",
      "  * Loading percentiles:  _2\n",
      "Features loaded !\n",
      "Execution --- 33.45758032798767 seconds ---\n",
      "Train df:  Index(['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'var_6', 'var_7',\n",
      "       'var_8', 'var_9',\n",
      "       ...\n",
      "       'kurt_2', 'med_2', 'ma_2', 'perc_1_2', 'perc_5_2', 'perc_25_2',\n",
      "       'perc_50_2', 'perc_75_2', 'perc_95_2', 'perc_99_2'],\n",
      "      dtype='object', length=232)\n",
      "Test df:  Index(['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'var_6', 'var_7',\n",
      "       'var_8', 'var_9',\n",
      "       ...\n",
      "       'kurt_2', 'med_2', 'ma_2', 'perc_1_2', 'perc_5_2', 'perc_25_2',\n",
      "       'perc_50_2', 'perc_75_2', 'perc_95_2', 'perc_99_2'],\n",
      "      dtype='object', length=232)\n",
      "Number of Features:  232\n",
      "Number of Features:  232\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def feature_creation(df, idx, name_num = '_1'):\n",
    "    #data metrics\n",
    "    print('  * Loading new data metrics: ', name_num)\n",
    "    df['sum'+name_num] = df[idx].sum(axis=1)  \n",
    "    df['min'+name_num] = df[idx].min(axis=1)\n",
    "    df['max'+name_num] = df[idx].max(axis=1)\n",
    "    df['mean'+name_num] = df[idx].mean(axis=1)\n",
    "    df['std'+name_num] = df[idx].std(axis=1)\n",
    "    df['skew'+name_num] = df[idx].skew(axis=1)\n",
    "    df['kurt'+name_num] = df[idx].kurtosis(axis=1)\n",
    "    df['med'+name_num] = df[idx].median(axis=1)\n",
    "    #moving average\n",
    "    print('  * Loading moving average metric: ', name_num)\n",
    "    df['ma'+name_num] =  df[idx].apply(lambda x: np.ma.average(x), axis=1)\n",
    "    #percentiles\n",
    "    print('  * Loading percentiles: ', name_num)\n",
    "    df['perc_1'+name_num] =  df[idx].apply(lambda x: np.percentile(x, 1), axis=1)\n",
    "    df['perc_5'+name_num] =  df[idx].apply(lambda x: np.percentile(x, 5), axis=1)\n",
    "    df['perc_25'+name_num] =  df[idx].apply(lambda x: np.percentile(x, 25), axis=1)\n",
    "    df['perc_50'+name_num] =  df[idx].apply(lambda x: np.percentile(x, 50), axis=1)\n",
    "    df['perc_75'+name_num] =  df[idx].apply(lambda x: np.percentile(x, 75), axis=1)\n",
    "    df['perc_95'+name_num] =  df[idx].apply(lambda x: np.percentile(x, 95), axis=1)\n",
    "    df['perc_99'+name_num] =  df[idx].apply(lambda x: np.percentile(x, 99), axis=1)\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#for i,df in enumerate([df_train_small, df_test_small]):\n",
    "print('Loading more features for train dataset')\n",
    "print('Creating Metrics Part 1')\n",
    "#features_1 = df_train_small.columns.values[2:202]\n",
    "features_1 = df_train_small.columns\n",
    "feature_creation(df_train_small, features_1, name_num='_1') #adding columns using the train features (#200)\n",
    "print('Creating Metrics Part 2')\n",
    "#features_2 = df_train_small.columns.values[2:218] #all features included the ones added\n",
    "features_2 = df_train_small.columns #all features included the ones added\n",
    "feature_creation(df_train_small, features_2, name_num='_2') #adding columns using the train features + the new features\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "print('Loading more features for test dataset')\n",
    "print('Creating Metrics Part 1')\n",
    "features_1 = df_test_small.columns\n",
    "feature_creation(df_test_small, features_1, name_num='_1') #adding columns using the train features (#200)\n",
    "print('Creating Metrics Part 2')\n",
    "features_2 = df_test_small.columns #all features included the ones added\n",
    "feature_creation(df_test_small, features_2, name_num='_2') #adding columns using the train features + the new features\n",
    "\n",
    "\n",
    "print('Features loaded !')\n",
    "print(\"Execution --- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Train df: ', df_train_small.columns)\n",
    "print('Test df: ', df_test_small.columns)\n",
    "print('Number of Features: ', len(df_train_small.columns))\n",
    "print('Number of Features: ', len(df_test_small.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 230)\n",
      "(5000, 230)\n"
     ]
    }
   ],
   "source": [
    "df_train_small.drop(['min_2','max_2'],axis=1,inplace=True)\n",
    "df_test_small.drop(['min_2','max_2'],axis=1,inplace=True)\n",
    "\n",
    "print(df_train_small.shape)\n",
    "print(df_test_small.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>kurt_2</th>\n",
       "      <th>med_2</th>\n",
       "      <th>ma_2</th>\n",
       "      <th>perc_1_2</th>\n",
       "      <th>perc_5_2</th>\n",
       "      <th>perc_25_2</th>\n",
       "      <th>perc_50_2</th>\n",
       "      <th>perc_75_2</th>\n",
       "      <th>perc_95_2</th>\n",
       "      <th>perc_99_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>13.4829</td>\n",
       "      <td>-2.3449</td>\n",
       "      <td>8.8573</td>\n",
       "      <td>9.0543</td>\n",
       "      <td>11.5035</td>\n",
       "      <td>-18.6207</td>\n",
       "      <td>5.6302</td>\n",
       "      <td>20.0854</td>\n",
       "      <td>1.6109</td>\n",
       "      <td>6.7838</td>\n",
       "      <td>...</td>\n",
       "      <td>211.844078</td>\n",
       "      <td>6.826325</td>\n",
       "      <td>14.004250</td>\n",
       "      <td>-17.371325</td>\n",
       "      <td>-8.960975</td>\n",
       "      <td>1.390275</td>\n",
       "      <td>6.826325</td>\n",
       "      <td>12.948225</td>\n",
       "      <td>25.536375</td>\n",
       "      <td>34.818080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923</th>\n",
       "      <td>8.8224</td>\n",
       "      <td>-10.0485</td>\n",
       "      <td>12.7912</td>\n",
       "      <td>2.6548</td>\n",
       "      <td>12.5755</td>\n",
       "      <td>-8.4661</td>\n",
       "      <td>5.7408</td>\n",
       "      <td>21.1540</td>\n",
       "      <td>-1.9907</td>\n",
       "      <td>5.6307</td>\n",
       "      <td>...</td>\n",
       "      <td>210.527171</td>\n",
       "      <td>5.754825</td>\n",
       "      <td>12.860593</td>\n",
       "      <td>-27.423560</td>\n",
       "      <td>-7.296450</td>\n",
       "      <td>0.742350</td>\n",
       "      <td>5.754825</td>\n",
       "      <td>13.100706</td>\n",
       "      <td>22.322550</td>\n",
       "      <td>34.033170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>18.7685</td>\n",
       "      <td>-7.3518</td>\n",
       "      <td>14.3081</td>\n",
       "      <td>7.0872</td>\n",
       "      <td>9.8062</td>\n",
       "      <td>-9.6050</td>\n",
       "      <td>5.7641</td>\n",
       "      <td>13.2822</td>\n",
       "      <td>2.8915</td>\n",
       "      <td>7.3591</td>\n",
       "      <td>...</td>\n",
       "      <td>210.934772</td>\n",
       "      <td>7.081825</td>\n",
       "      <td>14.312920</td>\n",
       "      <td>-26.469462</td>\n",
       "      <td>-9.855400</td>\n",
       "      <td>2.446850</td>\n",
       "      <td>7.081825</td>\n",
       "      <td>12.566100</td>\n",
       "      <td>26.710575</td>\n",
       "      <td>38.120495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        var_0    var_1    var_2   var_3    var_4    var_5   var_6    var_7  \\\n",
       "2729  13.4829  -2.3449   8.8573  9.0543  11.5035 -18.6207  5.6302  20.0854   \n",
       "3923   8.8224 -10.0485  12.7912  2.6548  12.5755  -8.4661  5.7408  21.1540   \n",
       "320   18.7685  -7.3518  14.3081  7.0872   9.8062  -9.6050  5.7641  13.2822   \n",
       "\n",
       "       var_8   var_9    ...          kurt_2     med_2       ma_2   perc_1_2  \\\n",
       "2729  1.6109  6.7838    ...      211.844078  6.826325  14.004250 -17.371325   \n",
       "3923 -1.9907  5.6307    ...      210.527171  5.754825  12.860593 -27.423560   \n",
       "320   2.8915  7.3591    ...      210.934772  7.081825  14.312920 -26.469462   \n",
       "\n",
       "      perc_5_2  perc_25_2  perc_50_2  perc_75_2  perc_95_2  perc_99_2  \n",
       "2729 -8.960975   1.390275   6.826325  12.948225  25.536375  34.818080  \n",
       "3923 -7.296450   0.742350   5.754825  13.100706  22.322550  34.033170  \n",
       "320  -9.855400   2.446850   7.081825  12.566100  26.710575  38.120495  \n",
       "\n",
       "[3 rows x 230 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df_train_small\n",
    "y = df_train['target'][:5000]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 13, test_size = 0.2)\n",
    "\n",
    "pd.DataFrame(data=x_train).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "GNB = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.36234774e-01 6.37652261e-02]\n",
      " [9.89847818e-01 1.01521817e-02]\n",
      " [9.96609332e-01 3.39066795e-03]\n",
      " [9.97655892e-01 2.34410831e-03]\n",
      " [9.99523643e-01 4.76356988e-04]\n",
      " [7.48078476e-01 2.51921524e-01]\n",
      " [9.80769004e-01 1.92309959e-02]\n",
      " [9.99757434e-01 2.42565769e-04]\n",
      " [9.67856200e-01 3.21438005e-02]\n",
      " [9.84074483e-01 1.59255174e-02]]\n"
     ]
    }
   ],
   "source": [
    "GNB.fit(x_train, y_train)\n",
    "y_preds_test = GNB.predict_proba(x_test)\n",
    "print(y_preds_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8215113424895252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score \n",
    "\n",
    "probs_pos_test_gnb  = []\n",
    "for pred in y_preds_test:\n",
    "    probs_pos_test_gnb.append(pred[1])\n",
    "    \n",
    "roc_test = roc_auc_score(y_test, probs_pos_test_gnb)\n",
    "print(roc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "\n",
    "# learning_rate': 0.008, n_estimators = 2000, commented out settings 1. --> 0.8836\n",
    "# learning_rate': 0.05, n_estimators = 2000, commented out settings 2. --> 0.8867\n",
    "# learning_rate': 0.07, n_estimators = 6000, commented out settings 2. --> 0.8649\n",
    "# learning_rate': 0.03, n_estimators = 2000, commented out settings 3. --> 0.8768\n",
    "# learning_rate': 0.03, n_estimators = 2000, commented out settings 4. --> 0.8851\n",
    "# learning_rate': 0.03, n_estimators = 2000, commented out settings 4. with gamma dist --> 0.8019\n",
    "# learning_rate': 0.03, n_estimators = 2000, commented out settings 5. with binary dist --> 0.8929\n",
    "# learning_rate': 0.03, n_estimators = 2000, commented out settings 5. with binary dist + classifier model --> 0.8930\n",
    "# learning_rate': 0.05, n_estimators = 2000, commented out settings 6. with binary dist + classifier model --> 0.8585\n",
    "# learning_rate': 0.02, n_estimators = 4000, commented out settings 7. with binary dist + classifier model --> 0.8950\n",
    "\n",
    "\n",
    "# 7\n",
    "LGB_params = {'num_leaves': 255,\n",
    "         'objective': 'binary',\n",
    "         'learning_rate': 0.02,\n",
    "         'metric': 'rmse',\n",
    "         'max_bin': 120,\n",
    "         'num_iterations': 4000,\n",
    "         'boosting': 'gbdt'}\n",
    "\n",
    "#6\n",
    "#LGB_params = {'num_leaves': 255,\n",
    "#         'min_data_in_leaf': 0,\n",
    "#         'objective': 'regression',\n",
    "#         'max_depth': 13,\n",
    "#         'learning_rate': 0.05,\n",
    "#         \"boosting\": \"gbdt\",\n",
    "#         \"feature_fraction\": 0.8,\n",
    "#         \"bagging_freq\": 7,\n",
    "#         \"bagging_fraction\": 0.8,\n",
    "#         \"bagging_seed\": 11,\n",
    "#         \"metric\": 'rmse',\n",
    "#         \"lambda_l1\": 0.1,\n",
    "#         \"verbosity\": -1,\n",
    "#         'min_child_weight': 5,\n",
    "#         'reg_alpha': 1.15,\n",
    "#         'reg_lambda': 0.25,\n",
    "#         'subsample': 0.80,\n",
    "#         'max_bin': 120}\n",
    "\n",
    "# 5\n",
    "#LGB_params = {'num_leaves': 255,\n",
    "#         'objective': 'binary',\n",
    "#         'learning_rate': 0.03,\n",
    "#         'metric': 'rmse',\n",
    "#         'max_bin': 120,\n",
    "#         'num_iterations': 2000,\n",
    "#         'boosting': 'gbdt'}\n",
    "\n",
    "# 4\n",
    "#LGB_params = {'num_leaves': 60,\n",
    "#         'objective': 'regression',\n",
    "#         'learning_rate': 0.03,\n",
    "#         'metric': 'rmse',\n",
    "#         'max_bin': 120,\n",
    "#         'num_iterations': 2000,\n",
    "#         'boosting': 'gbdt'}\n",
    "\n",
    "# 3\n",
    "#LGB_params = {'num_leaves': 60,\n",
    "#         'objective': 'regression',\n",
    "#         'learning_rate': 0.03,\n",
    "#         'metric': 'rmse',\n",
    "#         'max_bin': 120,\n",
    "#         'num_iterations': 2000,\n",
    "#         'boosting': 'dart'}\n",
    "\n",
    "# 2\n",
    "#LGB_params = {'num_leaves': 40,\n",
    "#         'min_data_in_leaf': 50,\n",
    "#         'objective': 'regression',\n",
    "#         'max_depth': 9,\n",
    "#         'learning_rate': 0.07,\n",
    "#         \"boosting\": \"gbdt\",\n",
    "#         \"feature_fraction\": 0.7,\n",
    "#         \"bagging_freq\": 7,\n",
    "#         \"bagging_fraction\": 0.7,\n",
    "#         \"bagging_seed\": 11,\n",
    "#         \"metric\": 'rmse',\n",
    "#         \"lambda_l1\": 0.1,\n",
    "#         \"verbosity\": -1,\n",
    "#         'min_child_weight': 5,\n",
    "#         'reg_alpha': 1.15,\n",
    "#         'reg_lambda': 0.25,\n",
    "#         'subsample': 0.80,}\n",
    "\n",
    "# 1. LGB_params = {'num_leaves': 50,\n",
    "#         'min_data_in_leaf': 70,\n",
    "#         'objective': 'regression',\n",
    "#         'max_depth': 13,\n",
    "#         'learning_rate': 0.008,\n",
    "#         \"boosting\": \"gbdt\",\n",
    "#         \"feature_fraction\": 0.8,\n",
    "#         \"bagging_freq\": 7,\n",
    "#         \"bagging_fraction\": 0.8,\n",
    "#         \"bagging_seed\": 11,\n",
    "#         \"metric\": 'rmse',\n",
    "#         \"lambda_l1\": 0.1,\n",
    "#         \"verbosity\": -1,\n",
    "#         'min_child_weight': 5,\n",
    "#         'reg_alpha': 1.15,\n",
    "#         'reg_lambda': 0.25,\n",
    "#         'subsample': 0.80,}\n",
    "\n",
    "#LGB = LGBMRegressor(**LGB_params, n_estimators = 2000, nthread = 4, n_jobs = -1)\n",
    "\n",
    "LGB = LGBMClassifier(**LGB_params, n_estimators = 2000, nthread = 4, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting='gbdt', boosting_type='gbdt', class_weight=None,\n",
       "        colsample_bytree=1.0, importance_type='split', learning_rate=0.02,\n",
       "        max_bin=120, max_depth=-1, metric='rmse', min_child_samples=20,\n",
       "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=2000,\n",
       "        n_jobs=-1, nthread=4, num_iterations=4000, num_leaves=255,\n",
       "        objective='binary', random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB.fit(x_train, y_train, eval_metric='rmse', verbose=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99975128e-01 2.48724166e-05]\n",
      " [9.99882493e-01 1.17507147e-04]\n",
      " [9.99998535e-01 1.46511896e-06]\n",
      " [9.99997042e-01 2.95754297e-06]\n",
      " [9.99999910e-01 8.96699629e-08]\n",
      " [9.99999026e-01 9.74082702e-07]\n",
      " [9.99998667e-01 1.33335200e-06]\n",
      " [9.99999945e-01 5.51616288e-08]\n",
      " [9.99991069e-01 8.93077808e-06]\n",
      " [9.99999180e-01 8.19861287e-07]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = LGB.predict_proba(x_test) # for classifier\n",
    "#y_pred = LGB.predict(x_test) # for regression\n",
    "print(y_pred[:10])\n",
    "#print(min(y_pred))\n",
    "#print(max(y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8010297861652453\n"
     ]
    }
   ],
   "source": [
    "#print(y_pred[:, 1:])\n",
    "roc_test = roc_auc_score(y_test, y_pred[:, 1:].clip(0, 1))\n",
    "print(roc_test)\n",
    "# got 0.8846161338721976 with LGBMClassifier and LGB.predict_proba(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
